{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress PyTorch deprecation warnings from the transformers library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"`torch.utils._pytree._register_pytree_node` is deprecated\")\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for optimization\n",
    "MAX_SEQ_LENGTH = 64      # Reduced from 128\n",
    "BATCH_SIZE = 16          # Increased from 8\n",
    "GRADIENT_ACCUMULATION = 4 # Effective batch size = 16 Ã— 4 = 64\n",
    "USE_FP16 = True          # Enable mixed precision training\n",
    "NUM_EPOCHS = 1           # Keep as 1 for demonstration\n",
    "TEACHER_MODEL = 'distilgpt2'  # Smaller teacher model\n",
    "DATASET_SIZE_LIMIT = 5000    # Limit dataset size\n",
    "NUM_WORKERS = 2          # Parallel data loading\n",
    "EVAL_STEPS = 200         # Evaluate less frequently\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configure Hugging Face cache\n",
    "import os\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '0'\n",
    "os.environ['HF_HOME'] = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\")\n",
    "\n",
    "# Load tokenizer with error handling\n",
    "try:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(TEACHER_MODEL, local_files_only=False)\n",
    "    print(f\"Successfully loaded the {TEACHER_MODEL} tokenizer\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    print(\"Trying alternative model: gpt2\")\n",
    "    try:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2', local_files_only=False)\n",
    "        print(\"Successfully loaded the GPT-2 tokenizer\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Error loading alternative tokenizer: {e2}\")\n",
    "        raise Exception(\"Could not load any tokenizer.\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load a small text dataset and limit its size\n",
    "try:\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "    print(\"Successfully loaded the dataset\")\n",
    "    \n",
    "    # Limit dataset size for faster training\n",
    "    if len(dataset['train']) > DATASET_SIZE_LIMIT:\n",
    "        dataset['train'] = dataset['train'].select(range(DATASET_SIZE_LIMIT))\n",
    "    if len(dataset['validation']) > DATASET_SIZE_LIMIT // 10:\n",
    "        dataset['validation'] = dataset['validation'].select(range(DATASET_SIZE_LIMIT // 10))\n",
    "    \n",
    "    print(f\"Limited training dataset to {len(dataset['train'])} examples\")\n",
    "    print(f\"Limited validation dataset to {len(dataset['validation'])} examples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Creating a simple example dataset instead\")\n",
    "    # Create a simple dataset\n",
    "    example_texts = [\n",
    "        {\"text\": \"This is an example text for language modeling.\"},\n",
    "        {\"text\": \"Knowledge distillation helps create smaller models.\"},\n",
    "        {\"text\": \"AI research focuses on efficient model deployment.\"}\n",
    "    ] * 100\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    dataset = {\n",
    "        'train': Dataset.from_list(example_texts),\n",
    "        'validation': Dataset.from_list(example_texts[:10])\n",
    "    }\n",
    "\n",
    "# More efficient tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "# Tokenize datasets with batched processing\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['text'])\n",
    "\n",
    "# Convert to PyTorch Dataset\n",
    "train_dataset = tokenized_datasets['train'].with_format(\"torch\")\n",
    "eval_dataset = tokenized_datasets['validation'].with_format(\"torch\")\n",
    "\n",
    "# Create optimized DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher model (Using distilgpt2 by default for faster training)\n",
    "class TeacherLLM(nn.Module):\n",
    "    def __init__(self, model_name=TEACHER_MODEL):\n",
    "        super(TeacherLLM, self).__init__()\n",
    "        try:\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(model_name, local_files_only=False)\n",
    "            print(f\"Successfully loaded {model_name} model\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name} model: {e}\")\n",
    "            fallback_model = 'gpt2'\n",
    "            if model_name == fallback_model:\n",
    "                print(\"Creating a basic model from scratch\")\n",
    "                config = GPT2Config(vocab_size=len(tokenizer))\n",
    "                self.model = GPT2LMHeadModel(config)\n",
    "            else:\n",
    "                print(f\"Trying fallback model: {fallback_model}\")\n",
    "                try:\n",
    "                    self.model = GPT2LMHeadModel.from_pretrained(fallback_model, local_files_only=False)\n",
    "                    print(f\"Successfully loaded {fallback_model} model\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error loading fallback model: {e2}\")\n",
    "                    print(\"Creating a basic model from scratch\")\n",
    "                    config = GPT2Config(vocab_size=len(tokenizer))\n",
    "                    self.model = GPT2LMHeadModel(config)\n",
    "        \n",
    "        self.model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, max_batches=None):\n",
    "    \"\"\"Evaluate model with option to limit evaluation batches\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if max_batches and i >= max_batches:\n",
    "                break\n",
    "                \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            with autocast(enabled=USE_FP16):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n",
    "            \n",
    "            batch_size = input_ids.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else float('inf')\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherLLM().to(device)\n",
    "teacher_optimizer = optim.AdamW(teacher_model.parameters(), lr=5e-5)\n",
    "teacher_scaler = GradScaler(enabled=USE_FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional fine-tuning with optimized setup\n",
    "num_epochs = 1  # Minimized for speed\n",
    "teacher_steps = 0  # Track total steps\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    teacher_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Teacher Training Epoch {epoch+1}\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Limit steps if needed (can uncomment to make even faster)\n",
    "        # if step >= 100: break\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast(enabled=USE_FP16):\n",
    "            outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n",
    "            loss = loss / GRADIENT_ACCUMULATION  # Scale loss for accumulation\n",
    "        \n",
    "        # Mixed precision backward pass\n",
    "        teacher_scaler.scale(loss).backward()\n",
    "        running_loss += loss.item() * GRADIENT_ACCUMULATION\n",
    "        \n",
    "        # Update weights with gradient accumulation\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION == 0 or (step + 1) == len(train_dataloader):\n",
    "            teacher_scaler.step(teacher_optimizer)\n",
    "            teacher_scaler.update()\n",
    "            teacher_optimizer.zero_grad(set_to_none=True)  # More memory efficient\n",
    "            \n",
    "            # Update progress\n",
    "            progress_bar.set_postfix({\"loss\": running_loss / (step + 1)})\n",
    "        \n",
    "        teacher_steps += 1\n",
    "        \n",
    "        # Evaluate periodically rather than every epoch\n",
    "        if teacher_steps % EVAL_STEPS == 0:\n",
    "            # Limit eval batches for faster feedback\n",
    "            avg_loss, perplexity = evaluate(teacher_model, eval_dataloader, max_batches=5)\n",
    "            print(f\"\\nStep {teacher_steps}, Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "            teacher_model.train()\n",
    "            \n",
    "            # Clean up memory\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Final evaluation\n",
    "    avg_loss, perplexity = evaluate(teacher_model, eval_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader):.4f}, \",\n",
    "          f\"Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student model (even smaller than before)\n",
    "class StudentLLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentLLM, self).__init__()\n",
    "        # Create an even smaller GPT-2 configuration\n",
    "        small_config = GPT2Config(\n",
    "            vocab_size=len(tokenizer),\n",
    "            n_positions=MAX_SEQ_LENGTH * 2,  # Smaller position embeddings\n",
    "            n_embd=256,     # Even smaller embedding (was 384)\n",
    "            n_layer=4,      # Fewer layers (was 6)\n",
    "            n_head=4        # Fewer attention heads (was 6)\n",
    "        )\n",
    "        self.model = GPT2LMHeadModel(small_config)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = StudentLLM().to(device)\n",
    "student_optimizer = optim.AdamW(student_model.parameters(), lr=1e-4)  # Slightly higher learning rate\n",
    "student_scaler = GradScaler(enabled=USE_FP16)  # Separate scaler for student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kd_loss_fn(student_logits, teacher_logits, labels, temperature=2.0, alpha=0.5):\n",
    "    \"\"\"Knowledge Distillation Loss with improved efficiency\"\"\"\n",
    "    # Apply temperature scaling to logits\n",
    "    soft_student = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    soft_teacher = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    \n",
    "    # Calculate KL divergence between student and teacher\n",
    "    # Use sum reduction for better numerical stability\n",
    "    kd_loss = F.kl_div(soft_student.view(-1, soft_student.size(-1)), \n",
    "                       soft_teacher.view(-1, soft_teacher.size(-1)), \n",
    "                       reduction='sum') * (temperature ** 2)\n",
    "    kd_loss = kd_loss / soft_student.size(0)  # Normalize by batch size\n",
    "    \n",
    "    # Create mask for valid tokens (not padding)\n",
    "    mask = (labels != -100).float()\n",
    "    num_valid_tokens = mask.sum()\n",
    "    \n",
    "    # Calculate the standard cross-entropy loss for the student\n",
    "    hard_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), \n",
    "                               labels.view(-1), \n",
    "                               ignore_index=-100)\n",
    "    \n",
    "    # Combine the two losses\n",
    "    loss = alpha * hard_loss + (1 - alpha) * kd_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized student training with early stopping\n",
    "num_epochs = 1\n",
    "temperature = 2.0\n",
    "alpha = 0.5\n",
    "patience = 3  # Early stopping patience\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "student_steps = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Student Training Epoch {epoch+1}\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Limit steps if needed (uncomment to make even faster)\n",
    "        # if step >= 200: break\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        with autocast(enabled=USE_FP16):\n",
    "            # Get student outputs\n",
    "            student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            student_logits = student_outputs.logits if hasattr(student_outputs, 'logits') else student_outputs[0]\n",
    "            \n",
    "            # Get teacher outputs (without gradient tracking)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                teacher_logits = teacher_outputs.logits if hasattr(teacher_outputs, 'logits') else teacher_outputs[0]\n",
    "            \n",
    "            # Calculate knowledge distillation loss\n",
    "            loss = kd_loss_fn(student_logits, teacher_logits, labels, temperature=temperature, alpha=alpha)\n",
    "            loss = loss / GRADIENT_ACCUMULATION  # Scale for accumulation\n",
    "        \n",
    "        # Backward pass with mixed precision\n",
    "        student_scaler.scale(loss).backward()\n",
    "        running_loss += loss.item() * GRADIENT_ACCUMULATION\n",
    "        \n",
    "        # Update weights with gradient accumulation\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION == 0 or (step + 1) == len(train_dataloader):\n",
    "            student_scaler.step(student_optimizer)\n",
    "            student_scaler.update()\n",
    "            student_optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": running_loss / (step + 1)})\n",
    "        \n",
    "        student_steps += 1\n",
    "        \n",
    "        # Evaluate periodically\n",
    "        if student_steps % EVAL_STEPS == 0:\n",
    "            avg_loss, perplexity = evaluate(student_model, eval_dataloader, max_batches=5)\n",
    "            print(f\"\\nStep {student_steps}, Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "                # Could save best model here\n",
    "                # torch.save(student_model.state_dict(), \"best_student_model.pt\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at step {student_steps}\")\n",
    "                    break\n",
    "            \n",
    "            student_model.train()\n",
    "            \n",
    "            # Clean up memory\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Early stopping check for epoch\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Training stopped early due to no improvement\")\n",
    "        break\n",
    "    \n",
    "    # Final evaluation\n",
    "    avg_loss, perplexity = evaluate(student_model, eval_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader):.4f}, \",\n",
    "          f\"Eval Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model sizes\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "teacher_params = count_parameters(teacher_model)\n",
    "student_params = count_parameters(student_model)\n",
    "\n",
    "print(f\"Teacher model parameters: {teacher_params:,}\")\n",
    "print(f\"Student model parameters: {student_params:,}\")\n",
    "print(f\"Size reduction: {(1 - student_params/teacher_params)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized text generation with caching\n",
    "input_text = \"In this article, we discuss the importance of\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "def generate_text(model, prompt, max_length=50):\n",
    "    # Free up memory before generation\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    with torch.no_grad(), autocast(enabled=USE_FP16):\n",
    "        input_ids = prompt['input_ids']\n",
    "        attention_mask = prompt['attention_mask']\n",
    "        generated = model.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            use_cache=True,  # Enable KV caching for faster generation\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "# Time teacher model\n",
    "%time teacher_output = generate_text(teacher_model, inputs)\n",
    "print(f\"Teacher output:\\n{teacher_output}\\n\")\n",
    "\n",
    "# Time student model\n",
    "%time student_output = generate_text(student_model, inputs)\n",
    "print(f\"Student output:\\n{student_output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized memory usage comparison\n",
    "def check_model_memory(model_name, model):\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    \n",
    "    # Track memory before model forward pass\n",
    "    memory_before = torch.cuda.memory_allocated(device)\n",
    "    \n",
    "    # Run a forward pass with mixed precision\n",
    "    with autocast(enabled=USE_FP16):\n",
    "        _ = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    \n",
    "    # Get peak memory during the forward pass\n",
    "    memory_peak = torch.cuda.max_memory_allocated(device)\n",
    "    memory_used = memory_peak - memory_before\n",
    "    \n",
    "    print(f\"{model_name} memory usage: {memory_used / 1024**2:.2f} MB\")\n",
    "    \n",
    "check_model_memory(\"Teacher\", teacher_model)\n",
    "check_model_memory(\"Student\", student_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
